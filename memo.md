Kayla Imanzi
AI Usage in Building InBetween Therapy AI
December 2024

My journey building InBetween Therapy AI involved more adaptation than I anticipated. I initially planned to use Google AI Studio for code generation, but after encountering repeated errors where the preview function would consistently break, I switched to Lovable AI. I also used ChatGPT early in Phase 1 to brainstorm my document outline and understand technical requirements for backend architecture. Lovable AI generated 100% of my prototype's functional code: the complete React component structure, Tailwind CSS styling,, interactive modal system, and all the mock data. When I prompted for a "calming design appropriate for a mental health practice," the AI interpreted this emotional context and chose soft colors, generous whitespace, and a card-based layout without me specifying exact design specifications.

However, throughout the website creation with Lovable, I made sure to review the outputs critically and make sure the MVP makes sense. The quality of output depended entirely on how clearly I articulated requirements. My first dashboard prompt was too vague and resulted in a cluttered interface, so I refined it to emphasize simple and not overwhelming. When the AI initially generated scoring logic, all recommendations scored between 90-95, making them indistinguishable, so I had to explicitly request "more variation." Most importantly, the AI had no inherent understanding of healthcare context or ethical considerations. I made the choices to display only client initials and exclude any clinical notes.

I chose AI scheduling recommendations over other features like client matching or payment automation for three reasons: therapists express concerns about AI accessing clinical content but scheduling is administrative data, scheduling is visibly time-consuming so AI can demonstrate value quickly, and unlike features requiring complex databases, scheduling recommendations could be demonstrated with mock data. My Phase 1 PRD described comprehensive calendar integration and automated notifications, but I deliberately simplified the website to focus on the core question: "Can AI meaningfully help therapists make better scheduling decisions?" The feature connects to my value proposition by analyzing multiple factors simultaneously and preventing decision fatigue, but it doesn't fully deliver yet because it can't actually book appointments or integrate with existing tools.

The prototype currently uses mock data with client initials only and nothing is stored or transmitted. While this models privacy-conscious design, it's not HIPAA-compliant and cannot handle real patient information. I prioritized demonstrating AI value over building security infrastructure, which is acceptable for a prototype but would be irresponsible for deployment. Some potential biases Iâ€™m concerned about are: the AI assumes all therapists want balanced schedules, assumes clients have consistent preferences without accounting for varying work schedules or childcare needs, and prioritizes pattern matching without considering urgent mental health needs. Future versions should allow easy therapist overrides, collect explicit client preferences, and include urgency flags that deprioritize pattern matching during crises.

Regarding over-reliance, I designed the AI as an advisor, not a decision-maker, with visible explanations, manual approval steps, and concern flags. My biggest worry is that therapists might trust recommendations without considering the context only they know. The AI lacks this clinical judgment, which is why I made choices to limit what the AI does: it doesn't make final booking decisions, access clinical notes, override therapist judgment, or learn automatically without consent. These limits exist because mental health professionals are licensed experts and AI should support, not replace, their judgment.

My biggest surprise throughout this process was how much output quality depended on subjective terms like "calming" and "thoughtful." When I prompted for "scheduling logic," I got technically correct but robotic explanations. Adding "sound like a compassionate colleague" transformed the tone to be more appropriate. This taught me that AI doesn't understand context the way humans do, so it needs explicit guidance on tone and intent. 

If I could teach another founder one thing, it would be: prompt like you're delegating to a competent junior developer who knows how to code but doesn't understand your business. Describe outcomes rather than implementations, include context about users and emotions, and iterate in small steps. This project taught me that AI accelerates prototyping but doesn't replace strategy.I was able to build a functional prototype in hours instead of weeks, but I still had to decide what problem to solve and how to build user trust. The AI didn't warn me about privacy risks or bias potential; it just uilt exactly what I asked for. As a founder, I'm responsible for anticipating misuse and unintended consequences.

I'm now more confident using AI for rapid prototyping but more cautious about deploying AI-powered products without rigorous testing and user feedback. If I pursue this beyond the course, my next steps would include user interviews with real therapists and possibly consultation with a healthcare attorney on HIPAA compliance. Overall, I think GenAI tools make it possible for non-technical founders to build functional products, but they don't reduce the responsibility to build thoughtfully and ethically.


